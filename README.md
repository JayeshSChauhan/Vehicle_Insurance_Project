# Vehicle Insurance ML Pipeline

A complete endâ€‘toâ€‘end Machine Learning pipeline for vehicle insurance claim prediction, from data ingestion through model training, evaluation, and deployment.

---

## ğŸš€ Features

- **Data Ingestion & Validation**  
  Fetch raw data, validate against schema, log issues.

- **Data Transformation**  
  Clean, featureâ€‘engineer, and prepare datasets for training.

- **Model Training & Evaluation**  
  Train your model, evaluate performance metrics, generate reports.

- **Model Pushing**  
  Serialize and upload the trained model to S3â€‘compatible storage.

- **Flask Demo**  
  A simple REST API (`app.py`) to serve predictions.

- **Notebooks**  
  - `notebook/exp-notebook.ipynb` â€” exploratory analysis  
  - `notebook/mongoDB_demo.ipynb` â€” demo of MongoDB integration  

---

## ğŸ“‚ Project Structure

```
Vehicle_Insurance_Project/
â”œâ”€â”€ artifact/              # All generated artifacts: models, reports, artifacts
â”œâ”€â”€ config/                
â”‚   â”œâ”€â”€ model.yaml         # Training parameters
â”‚   â””â”€â”€ schema.yaml        # Data schema definitions
â”œâ”€â”€ logs/                  # Log files (Pipeline runs, errors, etc.)
â”œâ”€â”€ notebook/              
â”‚   â”œâ”€â”€ data.csv           # Sample data CSV  
â”‚   â”œâ”€â”€ exp-notebook.ipynb 
â”‚   â””â”€â”€ mongoDB_demo.ipynb 
â”œâ”€â”€ src/                   
â”‚   â”œâ”€â”€ components/        # Core pipeline modules
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ data_validation.py
â”‚   â”‚   â”œâ”€â”€ data_transformation.py
â”‚   â”‚   â”œâ”€â”€ model_trainer.py
â”‚   â”‚   â”œâ”€â”€ model_evaluation.py
â”‚   â”‚   â””â”€â”€ model_pusher.py
â”‚   â”œâ”€â”€ cloud_storage/     # S3â€‘compatible upload logic
â”‚   â”‚   â””â”€â”€ aws_storage.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ static/                # Static assets (HTML, CSS)
â”œâ”€â”€ templates/             # Flask HTML templates
â”œâ”€â”€ app.py                 # Flask API for online predictions
â”œâ”€â”€ demo.py                # Quick demo script
â”œâ”€â”€ structure.py           # Orchestrator: runs full pipeline
â”œâ”€â”€ setup.py               # Project packaging info
â””â”€â”€ .dockerignore          # Docker ignore rules
```

---

## âš™ï¸ Prerequisites

- PythonÂ 3.8Â or higher  
- `pip`  

Install dependencies:
```bash
pip install -r requirements.txt
```

---

## ğŸ”§ Configuration

1. **`config/schema.yaml`**  
   Defines input data schema (columns, types, nullâ€‘checks).

2. **`config/model.yaml`**  
   Training hyperparameters:
   ```yaml
   train_test_split: 0.2
   random_state: 42
   model:
     type: XGBoost
     params:
       n_estimators: 100
       learning_rate: 0.1
   ```

3. **Cloud Storage Credentials**  
   Update your environment variables (or a `.env` file) for S3â€‘compatible upload:
   ```bash
   export AWS_ACCESS_KEY_ID="..."
   export AWS_SECRET_ACCESS_KEY="..."
   export AWS_ENDPOINT_URL="https://s3.us-west-004.backblazeb2.com"
   export AWS_REGION="us-west-004"
   ```

---

## ğŸƒâ€â™‚ï¸ Running the Pipeline

To run the **full pipeline** (ingestion â†’ validation â†’ training â†’ evaluation â†’ pushing):
```bash
python structure.py
```

- Artifacts (models, reports) appear under `artifact/`.
- Logs are written to `logs/`.

---

## ğŸ” Exploratory Notebooks

Launch Jupyter and open:
```bash
jupyter notebook notebook/exp-notebook.ipynb
jupyter notebook notebook/mongoDB_demo.ipynb
```

---

## ğŸ’» Flask Demo

Start the API server:
```bash
python app.py
```

- **POST** JSON payload to `http://localhost:5000/predict`  
- Returns model predictions as JSON.

Use the included `demo.py` for a quick example:
```bash
python demo.py
```

---

## ğŸ“¦ Packaging

This repo uses `setup.py` so you can install it as a package:
```bash
pip install -e .
```

---

## ğŸ“ˆ Logs & Monitoring

- Logs are under `logs/`  
- Check `logs/pipeline.log` for runâ€‘byâ€‘run status.

---

## ğŸ“„ License

Distributed under the MIT License. See [LICENSE](LICENSE) for more details.

---

## ğŸ™ Acknowledgements

- Inspired by standard MLOps best practices  
- Mentorâ€™s original AWSâ€‘based implementation  
- Uses Backblaze B2 as S3â€‘compatible storage  

---

**Enjoy!** ğŸ‰ Feel free to open an issue or drop a â­ if you find this useful.
